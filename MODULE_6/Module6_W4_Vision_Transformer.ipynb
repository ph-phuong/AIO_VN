{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3HQD0do3inp"
      },
      "source": [
        "## Kiến trúc Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOCimfFgavaW",
        "outputId": "41e5fdf0-f24c-4ccc-9b90-258e22691cf5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import các thư viện cần thiết\n",
        "\n",
        "import torch # type: ignore\n",
        "import torchvision.transforms as transforms # type: ignore\n",
        "from torch.utils.data import DataLoader, random_split # type: ignore\n",
        "import torch.optim as optim # type: ignore\n",
        "from torchvision.datasets import ImageFolder # type: ignore\n",
        "from torch import nn # type: ignore\n",
        "import math # type: ignore\n",
        "import os # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Gcu-fRXvQw"
      },
      "outputs": [],
      "source": [
        "# 1. Input Embedding, Positional Encoding\n",
        "class TokenAndPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_length, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.word_emb = nn.Embedding(\n",
        "                                    num_embeddings = vocab_size,\n",
        "                                    embedding_dim = embed_dim\n",
        "                                    )\n",
        "        self.pos_emb = nn.Embedding(\n",
        "                                    num_embeddings = max_length,\n",
        "                                    embedding_dim = embed_dim\n",
        "                                    )\n",
        "    def forward(self, x):\n",
        "        N, seq_len = x.size()\n",
        "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
        "        output1 = self.word_emb(x)\n",
        "        output2 = self.pos_emb(positions)\n",
        "        output = output1 + output2\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y_sQP2deLc4"
      },
      "outputs": [],
      "source": [
        "# 2. Encoder\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "                                        embed_dim = embed_dim,\n",
        "                                        num_heads = num_heads,\n",
        "                                        batch_first = True\n",
        "                                    )\n",
        "        self.ffn = nn.Sequential(\n",
        "                                nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "                                )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        att_output, _ = self.attn(query=query, key=key, value=value)\n",
        "        att_output = self.dropout_1(att_output)\n",
        "        out_1 = self.layernorm_1(query + att_output)\n",
        "        ffn_output = self.ffn(out_1)\n",
        "        ffn_output = self.dropout_2(ffn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + ffn_output)\n",
        "        return out_2\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 embed_dim,\n",
        "                 max_length,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 ff_dim,\n",
        "                 dropout=0.1,\n",
        "                 device='cpu'):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionalEmbedding(src_vocab_size,\n",
        "                                                     embed_dim,\n",
        "                                                     max_length,\n",
        "                                                     device)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderBlock(\n",
        "                            embed_dim, num_heads, ff_dim, dropout) for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, output, output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52ZQ4O1Dx5jI"
      },
      "outputs": [],
      "source": [
        "# 3. Decoder\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "                                        embed_dim = embed_dim,\n",
        "                                        num_heads = num_heads,\n",
        "                                        batch_first = True\n",
        "                                        )\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "                                                embed_dim = embed_dim,\n",
        "                                                num_heads = num_heads,\n",
        "                                                batch_first = True\n",
        "                                                )\n",
        "        self.ffn = nn.Sequential(\n",
        "                                nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "                                )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_3 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "        self.dropout_3 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        att_output, _ = self.attn(x, x, x, attn_mask=tgt_mask)\n",
        "        att_output = self.dropout_1(att_output)\n",
        "        out_1 = self.layernorm_1(x + att_output)\n",
        "\n",
        "        attn_output, _ = self.cross_attn(out_1, enc_output, enc_output, attn_mask=src_mask)\n",
        "        attn_output = self.dropout_2(attn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out_2)\n",
        "        ffn_output = self.dropout_3(ffn_output)\n",
        "        out_3 = self.layernorm_3(out_2 + ffn_output)\n",
        "        return out_3\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 tgt_vocab_size,\n",
        "                 embed_dim,\n",
        "                 max_length,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 ff_dim,\n",
        "                 dropout=0.1,\n",
        "                 device='cpu'):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionalEmbedding(tgt_vocab_size,\n",
        "                                                     embed_dim,\n",
        "                                                     max_length,\n",
        "                                                     device)\n",
        "        self.layers = nn.ModuleList([TransformerDecoderBlock(\n",
        "                            embed_dim, num_heads, ff_dim, dropout) for i in range(num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        output = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_output, src_mask, tgt_mask)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JAaasrlxxSY"
      },
      "outputs": [],
      "source": [
        "# 4. Transformer\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 tgt_vocab_size,\n",
        "                 embed_dim,\n",
        "                 max_length,\n",
        "                 num_layers,\n",
        "                 num_heads,\n",
        "                 ff_dim,\n",
        "                 dropout=0.1,\n",
        "                 device='cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.encoder = TransformerEncoder(src_vocab_size,\n",
        "                                          embed_dim,\n",
        "                                          max_length,\n",
        "                                          num_layers,\n",
        "                                          num_heads,\n",
        "                                          ff_dim,\n",
        "                                          dropout=dropout,\n",
        "                                          device=device\n",
        "                                          )\n",
        "        self.decoder = TransformerDecoder(tgt_vocab_size,\n",
        "                                          embed_dim,\n",
        "                                          max_length,\n",
        "                                          num_layers,\n",
        "                                          num_heads,\n",
        "                                          ff_dim,\n",
        "                                          dropout=dropout,\n",
        "                                          device=device\n",
        "                                          )\n",
        "        self.fc = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_seq_len = src.shape[1]\n",
        "        tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "        src_mask = torch.zeros((src_seq_len, src_seq_len), device=self.device).type(torch.bool)\n",
        "        tgt_mask = torch.triu(torch.zeros((tgt_seq_len, tgt_seq_len), device=self.device)==1).transpose(0, 1)\n",
        "        tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
        "\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        enc_output = self.encoder(src)\n",
        "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvxlHynKksSC"
      },
      "outputs": [],
      "source": [
        "# 5. Thử nghiệm\n",
        "batch_size = 128\n",
        "src_vocab_size = 1000\n",
        "tgt_vocab_size = 2000\n",
        "embed_dim = 200\n",
        "max_length = 100\n",
        "num_layers = 2\n",
        "num_headers = 4\n",
        "ff_dim = 256\n",
        "\n",
        "model = Transformer(src_vocab_size,\n",
        "                    tgt_vocab_size,\n",
        "                    embed_dim,\n",
        "                    max_length,\n",
        "                    num_layers,\n",
        "                    num_headers,\n",
        "                    ff_dim)\n",
        "src = torch.randint(\n",
        "                    high=2,\n",
        "                    size=(batch_size, max_length),\n",
        "                    dtype=torch.int64\n",
        "                    )\n",
        "tgt = torch.randint(\n",
        "                    high=2,\n",
        "                    size=(batch_size, max_length),\n",
        "                    dtype=torch.int64\n",
        "                    )\n",
        "\n",
        "prediction = model(src, tgt)\n",
        "prediction.shape #batch_size x max)_length x tgt_vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQeS-e4Z3dLy"
      },
      "source": [
        "## Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg6PbiO4uzhg"
      },
      "outputs": [],
      "source": [
        "# 1. Load Dataset\n",
        "!gdown 1vSevps_hV5zhVf6aWuN8X7dd-qSAIgcc\n",
        "!unzip ./flower_photos.zip\n",
        "\n",
        "# load data\n",
        "data_patch = \"./flower_photos\"\n",
        "dataset = ImageFolder(root = data_patch)\n",
        "num_samples = len(dataset)\n",
        "classes = dataset.classes\n",
        "num_classes = len(dataset.classes)\n",
        "\n",
        "# split\n",
        "TRAIN_RATIO, VALID_RATIO = 0.8, 0.1\n",
        "n_train_examples = int(num_samples * TRAIN_RATIO)\n",
        "n_valid_examples = int(num_samples * VALID_RATIO)\n",
        "n_test_examples = num_samples - n_train_examples - n_valid_examples\n",
        "train_dataset, valid_dataset, test_dataset = random_split(\n",
        "                        dataset,\n",
        "                        [n_train_examples, n_valid_examples, n_test_examples]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7sNI9Guuznw"
      },
      "outputs": [],
      "source": [
        "# 2. Preprocessing\n",
        "IMG_SIZE = 224\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        transforms.RandomRotation(0.2),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "                        ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "                        ])\n",
        "\n",
        "# apply\n",
        "train_dataset.dataset.transform = train_transforms\n",
        "valid_dataset.dataset.transform = test_transforms\n",
        "test_dataset.dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFmVKa5Q6hYx"
      },
      "outputs": [],
      "source": [
        "# 3. Dataloader\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tran epoch\n",
        "import time\n",
        "\n",
        "def train_epoch(model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # compute loss\n",
        "        loss = criterion(predictions, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_acc += (predictions.argmax(1) == labels).sum().item()\n",
        "        total_count += labels.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f}\".format(\n",
        "                    epoch, idx, len(train_dataloader), total_acc / total_count\n",
        "                )\n",
        "            )\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "    epoch_acc = total_acc / total_count\n",
        "    epoch_loss = sum(losses) / len(losses)\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# evaluate\n",
        "def evaluate_epoch(model, criterion, dataloader, device):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for idx, (inputs, labels) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            predictions = model(inputs)\n",
        "\n",
        "            loss = criterion(predictions, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            total_acc += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "\n",
        "    epoch_acc = total_acc / total_count\n",
        "    epoch_loss = sum(losses) / len(losses)\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# train\n",
        "def train(model, model_name, save_model, optimizer, criterion, train_dataloader, val_dataloader, num_epochs, device):\n",
        "    train_accs, train_losses = [], []\n",
        "    eval_accs, eval_losses = [], []\n",
        "    best_loss_eval = 100\n",
        "    times = []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        # Training\n",
        "        train_acc, train_loss = train_epoch(model, optimizer, criterion, train_dataloader, device, epoch)\n",
        "        train_accs.append(train_acc)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        eval_acc, eval_loss = evaluate_epoch(model, criterion, val_dataloader, device)\n",
        "        eval_accs.append(eval_acc)\n",
        "        eval_losses.append(eval_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if eval_loss < best_loss_eval:\n",
        "            torch.save(model.state_dict(), save_model + f'/{model_name}.pt')\n",
        "\n",
        "        times.append(time.time() - epoch_start_time)\n",
        "\n",
        "        # Print loss, acc end epoch\n",
        "        print(\"-\" * 59)\n",
        "        print(\n",
        "            \"| End of epoch {:3d} | time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f}\"\n",
        "            \"| Valid Accuracy {:8.3f} | Valid Loss {:8.3f}\".format(\n",
        "                epoch, times.time() - epoch_start_time, train_acc, train_loss, eval_loss, eval_acc\n",
        "            )\n",
        "        )\n",
        "        print(\"-\" *59)\n",
        "\n",
        "    # Load beset model\n",
        "    model.load_state_dict(torch.load(save_model + f'/{model_name}.pt'))\n",
        "    model.eval()\n",
        "    metrics = {\n",
        "            'train_accuracy' : train_accs,\n",
        "            'train_loss' : train_losses,\n",
        "            'valid_accuracy' : eval_accs,\n",
        "            'valid_loss' : eval_losses,\n",
        "            'time' : times\n",
        "            }\n",
        "    return model, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSEROm62TyY1"
      },
      "outputs": [],
      "source": [
        "# 4. Training from Scratch\n",
        "# 4.1. Modeling\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "                                        embed_dim = embed_dim,\n",
        "                                        num_heads = num_heads,\n",
        "                                        batch_first = True\n",
        "                                        )\n",
        "        self.ffn = nn.Sequential(\n",
        "                                nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
        "                                )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        att_output, _ = self.attn(query, key, value)\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        out_1 = self.layernorm_1(query + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out_1)\n",
        "        ffn_output = self.dropout_2(ffn_output)\n",
        "        out_2 = self.layernorm_2(out_1 + ffn_output)\n",
        "        return out_2\n",
        "\n",
        "class PactchPositionEmbedding(nn.Module):\n",
        "    def __init__(self, image_size=224, embed_dim=512, patch_size=16, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "                            in_channels=3,\n",
        "                            out_channels=embed_dim,\n",
        "                            kernel_size=patch_size,\n",
        "                            stride=patch_size,\n",
        "                            bias=False\n",
        "                            )\n",
        "        scale = embed_dim ** -0.5\n",
        "        self.positional_embedding = nn.Parameter(scale * torch.randn((image_size // patch_size) ** 2, embed_dim))\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x = x + self.positional_embedding.to(self.device)\n",
        "        return x\n",
        "\n",
        "class VisionTransformerCls(nn.Module):\n",
        "    def __init__(self,\n",
        "                 image_size,\n",
        "                 embed_dim,\n",
        "                 num_heads,\n",
        "                 ff_dim,\n",
        "                 dropout=0.1,\n",
        "                 device='cpu',\n",
        "                 num_classes = 10,\n",
        "                 patch_size = 16):\n",
        "        super().__init__()\n",
        "        self.embd_layer = PactchPositionEmbedding(image_size=image_size,\n",
        "                                                  embed_dim=embed_dim,\n",
        "                                                  patch_size=patch_size,\n",
        "                                                  device=device\n",
        "                                                )\n",
        "        self.transformer_layer = TransformerEncoder(embed_dim, num_heads, ff_dim, dropout)\n",
        "\n",
        "        # self.pooling = nn.AvgPool1d(kernel_size=max_length)\n",
        "        self.fc1 = nn.Linear(in_features=embed_dim, out_features=20)\n",
        "        self.fc2 = nn.Linear(in_features=20, out_features=num_classes)\n",
        "        self.drouout = nn.Dropout(p=dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embd_layer(x)\n",
        "        output = self.transformer_layer(output, output, output)\n",
        "        output = output[:, 0, :]\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc1(output)\n",
        "        output = self.drouout(output)\n",
        "        output = self.fc2(output)\n",
        "        return output\n",
        "\n",
        "# 4.2. Training\n",
        "image_size = 224\n",
        "embed_dim = 512\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "dropout = 0.1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = VisionTransformerCls(image_size,\n",
        "                            embed_dim,\n",
        "                            num_heads,\n",
        "                            ff_dim,\n",
        "                            dropout,\n",
        "                            num_classes,\n",
        "                            device)\n",
        "model.to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
        "\n",
        "num_epochs = 100\n",
        "save_model = './model'\n",
        "os.makedirs(save_model, exist_ok=True)\n",
        "model_name = 'vit_flowers'\n",
        "\n",
        "model, metrics = train(\n",
        "                        model, model_name, save_model, optimizer, criterion, train_loader,\n",
        "                        valid_loader, num_epochs, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRPluqkE61KT"
      },
      "outputs": [],
      "source": [
        "# 5. Fine Tuning\n",
        "# 5.1. Modeling\n",
        "from transformers import ViTForImageClassification # type: ignore\n",
        "\n",
        "id2label = {id: label for id, label in enumerate(classes)}\n",
        "label2id = {label :id for id, label in id2label.items()}\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
        "                                                    num_labels = num_classes,\n",
        "                                                    id2label = id2label,\n",
        "                                                    label2id = label2id\n",
        "                                                  )\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# 5.2. Metric\n",
        "import evaluate # type: ignore\n",
        "import numpy as np # type: ignore\n",
        "\n",
        "metric = evaluate.Load('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# 5.3. Trainer\n",
        "\n",
        "import torch # type: ignore\n",
        "from transformers import ViTImageProcessor, TrainingArguments, Trainer # type: ignore\n",
        "\n",
        "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "metric_name = 'accuracy'\n",
        "\n",
        "args = TrainingArguments(\"vit_flowers\",\n",
        "                        save_strategy='epoch',\n",
        "                        evaluation_strategy='epoch',\n",
        "                        learning_rate=2e-5,\n",
        "                        per_device_train_batch_size =32,\n",
        "                        per_device_eval_batch_size =32,\n",
        "                        num_train_epochs =10,\n",
        "                        weight_decay =0.01,\n",
        "                        load_best_model_at_end =True,\n",
        "                        metric_for_best_model = metric_name,\n",
        "                        logging_dir ='logs',\n",
        "                        remove_unused_columns =False\n",
        "                         )\n",
        "\n",
        "def collate_fn(examples):\n",
        "    # example => Tuple(image, label)\n",
        "    pixel_values = torch.stack([example[0] for example in examples])\n",
        "    labels = torch.tensor([example[1] for example in examples])\n",
        "    return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': labels\n",
        "            }\n",
        "\n",
        "trainer = Trainer(\n",
        "                model,\n",
        "                args,\n",
        "                train_dataset = train_dataset,\n",
        "                eval_dataset = valid_dataset,\n",
        "                data_collator = collate_fn,\n",
        "                compute_metrics = compute_metrics,\n",
        "                tokenizer = feature_extractor\n",
        "                )\n",
        "\n",
        "# 5.4. Training\n",
        "trainer.train()\n",
        "outputs = trainer.predict(test_dataset)\n",
        "outputs.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNX85_Le6hho"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
