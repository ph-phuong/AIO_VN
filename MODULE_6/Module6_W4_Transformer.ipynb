{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3HQD0do3inp"
   },
   "source": [
    "## Kiến trúc Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOCimfFgavaW"
   },
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "\n",
    "import torch # type: ignore\n",
    "import torch.nn as nn # type: ignore\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import os # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import re # type: ignore\n",
    "import nltk # type: ignore\n",
    "\n",
    "nltk.download('stopwords') # type: ignore\n",
    "from nltk.corpus import stopwords # type: ignore\n",
    "from nltk.stem.porter import PorterStemmer # type: ignore\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7Gcu-fRXvQw"
   },
   "outputs": [],
   "source": [
    "# 1. Input Embedding, Positional Encoding\n",
    "class TokenAndPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_length, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_emb = nn.Embedding(\n",
    "                                    num_embeddings = vocab_size,\n",
    "                                    embedding_dim = embed_dim\n",
    "                                    )\n",
    "        self.pos_emb = nn.Embedding(\n",
    "                                    num_embeddings = max_length,\n",
    "                                    embedding_dim = embed_dim\n",
    "                                    )\n",
    "    def forward(self, x):\n",
    "        N, seq_len = x.size()\n",
    "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
    "        output1 = self.word_emb(x)\n",
    "        output2 = self.pos_emb(positions)\n",
    "        output = output1 + output2\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Y_sQP2deLc4"
   },
   "outputs": [],
   "source": [
    "# 2. Encoder\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "                                        embed_dim = embed_dim,\n",
    "                                        num_heads = num_heads,\n",
    "                                        batch_first = True\n",
    "                                    )\n",
    "        self.ffn = nn.Sequential(\n",
    "                                nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
    "                                )\n",
    "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
    "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        att_output, _ = self.attn(query=query, key=key, value=value)\n",
    "        att_output = self.dropout_1(att_output)\n",
    "        out_1 = self.layernorm_1(query + att_output)\n",
    "        ffn_output = self.ffn(out_1)\n",
    "        ffn_output = self.dropout_2(ffn_output)\n",
    "        out_2 = self.layernorm_2(out_1 + ffn_output)\n",
    "        return out_2\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 embed_dim,\n",
    "                 max_length,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 ff_dim,\n",
    "                 dropout=0.1,\n",
    "                 device='cpu'):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenAndPositionalEmbedding(src_vocab_size,\n",
    "                                                     embed_dim,\n",
    "                                                     max_length,\n",
    "                                                     device)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderBlock(\n",
    "                            embed_dim, num_heads, ff_dim, dropout) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, output, output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52ZQ4O1Dx5jI"
   },
   "outputs": [],
   "source": [
    "# 3. Decoder\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "                                        embed_dim = embed_dim,\n",
    "                                        num_heads = num_heads,\n",
    "                                        batch_first = True\n",
    "                                        )\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "                                                embed_dim = embed_dim,\n",
    "                                                num_heads = num_heads,\n",
    "                                                batch_first = True\n",
    "                                                )\n",
    "        self.ffn = nn.Sequential(\n",
    "                                nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n",
    "                                )\n",
    "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
    "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
    "        self.layernorm_3 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "        self.dropout_3 = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        att_output, _ = self.attn(x, x, x, attn_mask=tgt_mask)\n",
    "        att_output = self.dropout_1(att_output)\n",
    "        out_1 = self.layernorm_1(x + att_output)\n",
    "\n",
    "        attn_output, _ = self.cross_attn(out_1, enc_output, enc_output, attn_mask=src_mask)\n",
    "        attn_output = self.dropout_2(attn_output)\n",
    "        out_2 = self.layernorm_2(out_1 + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out_2)\n",
    "        ffn_output = self.dropout_3(ffn_output)\n",
    "        out_3 = self.layernorm_3(out_2 + ffn_output)\n",
    "        return out_3\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 tgt_vocab_size,\n",
    "                 embed_dim,\n",
    "                 max_length,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 ff_dim,\n",
    "                 dropout=0.1,\n",
    "                 device='cpu'):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenAndPositionalEmbedding(tgt_vocab_size,\n",
    "                                                     embed_dim,\n",
    "                                                     max_length,\n",
    "                                                     device)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(\n",
    "                            embed_dim, num_heads, ff_dim, dropout) for i in range(num_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        output = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, enc_output, src_mask, tgt_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JAaasrlxxSY"
   },
   "outputs": [],
   "source": [
    "# 4. Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 embed_dim,\n",
    "                 max_length,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 ff_dim,\n",
    "                 dropout=0.1,\n",
    "                 device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = TransformerEncoder(src_vocab_size,\n",
    "                                          embed_dim,\n",
    "                                          max_length,\n",
    "                                          num_layers,\n",
    "                                          num_heads,\n",
    "                                          ff_dim,\n",
    "                                          dropout=dropout,\n",
    "                                          device=device\n",
    "                                          )\n",
    "        self.decoder = TransformerDecoder(tgt_vocab_size,\n",
    "                                          embed_dim,\n",
    "                                          max_length,\n",
    "                                          num_layers,\n",
    "                                          num_heads,\n",
    "                                          ff_dim,\n",
    "                                          dropout=dropout,\n",
    "                                          device=device\n",
    "                                          )\n",
    "        self.fc = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[1]\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), device=self.device).type(torch.bool)\n",
    "        tgt_mask = torch.triu(torch.zeros((tgt_seq_len, tgt_seq_len), device=self.device)==1).transpose(0, 1)\n",
    "        tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
    "\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        enc_output = self.encoder(src)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvxlHynKksSC"
   },
   "outputs": [],
   "source": [
    "# 5. Thử nghiệm\n",
    "batch_size = 128\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 2000\n",
    "embed_dim = 200\n",
    "max_length = 100\n",
    "num_layers = 2\n",
    "num_headers = 4\n",
    "ff_dim = 256\n",
    "\n",
    "model = Transformer(src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    embed_dim,\n",
    "                    max_length,\n",
    "                    num_layers,\n",
    "                    num_headers,\n",
    "                    ff_dim)\n",
    "src = torch.randint(\n",
    "                    high=2,\n",
    "                    size=(batch_size, max_length),\n",
    "                    dtype=torch.int64\n",
    "                    )\n",
    "tgt = torch.randint(\n",
    "                    high=2,\n",
    "                    size=(batch_size, max_length),\n",
    "                    dtype=torch.int64\n",
    "                    )\n",
    "\n",
    "prediction = model(src, tgt)\n",
    "prediction.shape #batch_size x max)_length x tgt_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQeS-e4Z3dLy"
   },
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg6PbiO4uzhg"
   },
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "%pip install datasets\n",
    "\n",
    "from datasets import load_dataset # type: ignore\n",
    "\n",
    "ds = load_dataset('thainq107/ntc-scv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7sNI9Guuznw"
   },
   "outputs": [],
   "source": [
    "# 2. Preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove URLs https://www.\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\s+')\n",
    "    text = url_pattern.sub(r' ', text)\n",
    "\n",
    "    # remove HTML tags: <>\n",
    "    html_pattern = re.compile(r'[^<>]+>')\n",
    "    text = html_pattern.sub(r' ', text)\n",
    "\n",
    "    # remove punctuation and digits\n",
    "    replace_chars = list(string.punctuation + string.digits)\n",
    "    for char in replace_chars:\n",
    "        text = text.replace(char, ' ')\n",
    "\n",
    "    # remove emoji\n",
    "    emoji_pattern = re.compile (\"[\"\n",
    "                            u\"\\U0001F600 -\\U0001F64F\" # emoticons\n",
    "                            u\"\\U0001F300 -\\U0001F5FF\" # symbols & pictographs\n",
    "                            u\"\\U0001F680 -\\U0001F6FF\" # transport & map symbols\n",
    "                            u\"\\U0001F1E0 -\\U0001F1FF\" # flags (iOS)\n",
    "                            u\"\\U0001F1F2 -\\U0001F1F4\" # Macau flag\n",
    "                            u\"\\U0001F1E6 -\\U0001F1FF\" # flags\n",
    "                            u\"\\U0001F600 -\\U0001F64F\"\n",
    "                            u\"\\U00002702 -\\U000027B0\"\n",
    "                            u\"\\U000024C2 -\\U0001F251\"\n",
    "                            u\"\\U0001f926 -\\U0001f937\"\n",
    "                            u\"\\U0001F1F2\"\n",
    "                            u\"\\U0001F1F4\"\n",
    "                            u\"\\U0001F620\"\n",
    "                            u\"\\u200d\"\n",
    "                            u\"\\u2640 -\\u2642\"\n",
    "                            \"]+\", flags =re.UNICODE)\n",
    "    text = emoji_pattern.sub(r\" \", text)\n",
    "\n",
    "    # normalize whitespace\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # lowercasting\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKVYkl1Ae6nl"
   },
   "outputs": [],
   "source": [
    "# 3. Representation\n",
    "%pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ2W2gam64bZ"
   },
   "outputs": [],
   "source": [
    "def yield_tokens(sentences, tokenizer):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "# word-based tokenizer\n",
    "from torchtext.data import get_tokenizer # type: ignore\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# build vocabulary\n",
    "from torchtext.vocab import build_vocab_from_iterator # type: ignore\n",
    "\n",
    "vocab_size = 10000\n",
    "vocabulary = build_vocab_from_iterator(yield_tokens(ds['train']['preprocess_sentence'], tokenizer),\n",
    "                                       max_tokens=vocab_size,\n",
    "                                       specials=['<pad>', '<unk>'])\n",
    "vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "# convert torchtext dataset\n",
    "from torchtext.data.functial import to_map_style_dataset # type: ignore\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    # create iterator for dataset: (sentence, label)\n",
    "    for row in df:\n",
    "        sentence = row['preprocess_sentence']\n",
    "        encoded_sentence = vocabulary(tokenizer(sentence))\n",
    "        label = row['label']\n",
    "        yield sentence, label\n",
    "\n",
    "train_dataset = prepare_dataset(ds['train'])\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "\n",
    "val_dataset = prepare_dataset(ds['validation'])\n",
    "val_dataset = to_map_style_dataset(val_dataset)\n",
    "\n",
    "test_dataset = prepare_dataset(ds['test'])\n",
    "test_dataset = to_map_style_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFmVKa5Q6hYx"
   },
   "outputs": [],
   "source": [
    "# 4. Dataloader\n",
    "import torch # type: ignore\n",
    "seq_length = 100\n",
    "\n",
    "def collate_batch(batch):\n",
    "    #create inputs, offsets, labels for batch\n",
    "    sentences, labes = list(zip(*batch))\n",
    "    encoded_sentences = [\n",
    "                       sentence + ([0] * (seq_length-len(sentence))) if len(sentence) < seq_length else sentence[:seq_length]\n",
    "                       for sentence in sentences\n",
    "                       ]\n",
    "    encoded_sentences = torch.tensor(encoded_sentences, dtype=torch.int64)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "    return encoded_sentences, labels\n",
    "\n",
    "from torch.utils.data import DataLoader # type: ignore\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_batch,\n",
    "                          num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_batch,\n",
    "                        num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_batch,\n",
    "                        num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRPluqkE61KT"
   },
   "outputs": [],
   "source": [
    "# 5. Trainer\n",
    "# tran epoch\n",
    "import time\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(train_dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# evaluate\n",
    "def evaluate_epoch(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(inputs)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# train\n",
    "def train(model, model_name, save_model, optimizer, criterion, train_dataloader, val_dataloader, num_epochs, device):\n",
    "    train_accs, train_losses = [], []\n",
    "    eval_accs, eval_losses = [], []\n",
    "    best_loss_eval = 100\n",
    "    times = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training\n",
    "        train_acc, train_loss = train_epoch(model, optimizer, criterion, train_dataloader, device, epoch)\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluation\n",
    "        eval_acc, eval_loss = evaluate_epoch(model, criterion, val_dataloader, device)\n",
    "        eval_accs.append(eval_acc)\n",
    "        eval_losses.append(eval_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if eval_loss < best_loss_eval:\n",
    "            torch.save(model.state_dict(), save_model + f'/{model_name}.pt')\n",
    "\n",
    "        times.append(time.time() - epoch_start_time)\n",
    "\n",
    "        # Print loss, acc end epoch\n",
    "        print(\"-\" * 59)\n",
    "        print(\n",
    "            \"| End of epoch {:3d} | time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f}\"\n",
    "            \"| Valid Accuracy {:8.3f} | Valid Loss {:8.3f}\".format(\n",
    "                epoch, times.time() - epoch_start_time, train_acc, train_loss, eval_loss, eval_acc\n",
    "            )\n",
    "        )\n",
    "        print(\"-\" *59)\n",
    "\n",
    "    # Load beset model\n",
    "    model.load_state_dict(torch.load(save_model + f'/{model_name}.pt'))\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "            'train_accuracy' : train_accs,\n",
    "            'train_loss' : train_losses,\n",
    "            'valid_accuracy' : eval_accs,\n",
    "            'valid_loss' : eval_losses,\n",
    "            'time' : times\n",
    "            }\n",
    "    return model, metrics\n",
    "\n",
    "# Report\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "def plot_result(num_epochs, train_accs, eval_accs, train_losses, eval_losses):\n",
    "    epochs = list(range(num_epochs))\n",
    "    fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = (12.6))\n",
    "    axs[0].plot(epochs, train_accs, label = 'Training')\n",
    "    axs[0].plot(epochs, eval_accs, label = 'Evaluation')\n",
    "    axs[1].plot(epochs, train_losses, label = 'Training')\n",
    "    axs[1].plot(epochs, eval_losses, label = 'Evaluation')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[0].set_ytitle('Accuracy')\n",
    "    axs[1].set_ytitle('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFPE_lsi_Xqp"
   },
   "outputs": [],
   "source": [
    "# 6. Modeling\n",
    "class TransformerEncoderCls(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size, max_length, num_layers, embed_dim, num_heads, ff_dim,\n",
    "                 dropout=0.1,\n",
    "                 device='cpu'):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size,\n",
    "                                          embed_dim,\n",
    "                                          max_length,\n",
    "                                          num_layers,\n",
    "                                          num_heads,\n",
    "                                          ff_dim,\n",
    "                                          dropout,\n",
    "                                          device)\n",
    "        self.pooling = nn.AvgPool1d(kernel_size=max_length)\n",
    "        self.fc1 = nn.Linear(in_features=embed_dim, out_features=20)\n",
    "        self.fc2 = nn.Linear(in_features=20, out_features=2)\n",
    "        self.drouout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.encoder(x)\n",
    "        output = self.pooling(output.permute(0, 2, 1)).squeeze()\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc1(output)\n",
    "        output = self.drouout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "# 7. Training\n",
    "import torch.optim as optim # type: ignore\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "embed_dim = 200\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerEncoderCls(vocab_size,\n",
    "                              max_length,\n",
    "                              num_layers,\n",
    "                              embed_dim,\n",
    "                              num_heads,\n",
    "                              ff_dim,\n",
    "                              dropout\n",
    "                            )\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerEncoderCls(vocab_size,\n",
    "                              max_length,\n",
    "                              num_layers,\n",
    "                              embed_dim,\n",
    "                              num_heads,\n",
    "                              ff_dim,\n",
    "                              dropout,\n",
    "                              device\n",
    "                            )\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=0.0001)\n",
    "\n",
    "num_epochs = 50\n",
    "save_model = './model'\n",
    "os.makedirs(save_model, exist_ok=True)\n",
    "model_name = 'model'\n",
    "\n",
    "model, metrics = train(\n",
    "                        model, model_name, save_model, optimizer, criterion, train_dataloader,\n",
    "                        val_dataloader, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNX85_Le6hho"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
